{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReviewRatings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7yVTlplMgxRbdlyz9pGuA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikassinha167/Seldon/blob/master/ReviewRatings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CESEj_VshVcC",
        "outputId": "79e23fb1-2129-4f65-ffc3-7c9b0e1fecd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 27.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 25.9 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 74.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 77.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 79.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, fsspec, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.4.0 fsspec-2022.7.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQP14VFihFkw",
        "outputId": "c68ee8d0-4e5b-434d-e606-1fa0f56032ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to ./nltk...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to ./nltk...\n",
            "[nltk_data] Downloading package omw-1.4 to ./nltk...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, DefaultDataCollator, TFAutoModelForSequenceClassification\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "Path(\"1\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "nltk.download(\"stopwords\", download_dir=\"./nltk\")\n",
        "nltk.download(\"wordnet\", download_dir=\"./nltk\")\n",
        "nltk.download(\"omw-1.4\", download_dir=\"./nltk\")\n",
        "nltk.data.path.append(\"./nltk\")\n",
        "# Stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewRatings(object):\n",
        "    def __init__(self, model_path):\n",
        "        logger.info(\"Connecting to GCS\")\n",
        "        self.client = storage.Client.create_anonymous_client()\n",
        "        self.bucket = self.client.bucket('kelly-seldon')\n",
        "\n",
        "        logger.info(f\"Model name: {model_path}\")\n",
        "        self.model = None\n",
        "        self.prefix = model_path\n",
        "        self.local_dir = \"1/\"\n",
        "\n",
        "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        logger.info(\"Loading tokenizer and data collator\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "        self.ready = False\n",
        "\n",
        "    def load_model(self):\n",
        "        logger.info(\"Getting model artifact from GCS\")\n",
        "        blobs = self.bucket.list_blobs(prefix=self.prefix)\n",
        "        for blob in blobs:\n",
        "            filename = blob.name.split('/')[-1]\n",
        "            blob.download_to_filename(self.local_dir + filename)\n",
        "        logger.info(\"Loading model\")\n",
        "        self.model = TFAutoModelForSequenceClassification.from_pretrained(\"1\", num_labels=9)\n",
        "        logger.info(f\"{self.model.summary}\")\n",
        "\n",
        "    def preprocess_text(self, text, feature_names):\n",
        "        logger.info(\"Preprocessing text\")\n",
        "        logger.info(f\"Incoming text: {text}\")\n",
        "        text_list = text[0]\n",
        "        dict_text = {\"review\": text_list}\n",
        "        df = pd.DataFrame(data=dict_text)\n",
        "        logger.info(f\"Dataframe created: {df}\")\n",
        "        logger.info(\"Removing punctuation\")\n",
        "        df['review'] = df['review'].apply(lambda x: self.remove_punctuation(x))\n",
        "        logger.info(\"Lowercase all characters\")\n",
        "        df['review'] = df['review'].apply(lambda x: x.lower())\n",
        "        logger.info(\"Removing stopwords\")\n",
        "        df['review'] = df['review'].apply(lambda x: self.remove_stopwords(x))\n",
        "        logger.info(\"Carrying out lemmatization\")\n",
        "        df['review'] = df['review'].apply(lambda x: self.lemmatizer(x))\n",
        "\n",
        "        len_df = len(df)\n",
        "        logger.info(f\"{len(df)}\")\n",
        "\n",
        "        dataset = datasets.Dataset.from_pandas(df, preserve_index=False)\n",
        "        logger.info(f\"Dataset created: {dataset}\")\n",
        "\n",
        "        tokenized_revs = dataset.map(self.tokenize, batched=True)\n",
        "        logger.info(f\"Tokenized reviews: {tokenized_revs}\")\n",
        "\n",
        "        logger.info(\"Converting tokenized reviews to tf dataset\")\n",
        "        tf_inf = tokenized_revs.to_tf_dataset(\n",
        "            columns=[\"attention_mask\", \"input_ids\"],\n",
        "            label_cols=[\"labels\"],\n",
        "            shuffle=True,\n",
        "            batch_size=len_df,\n",
        "            collate_fn=self.data_collator\n",
        "        )\n",
        "        logger.info(f\"TF dataset created: {tf_inf}\")\n",
        "\n",
        "        return tf_inf\n",
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        punctuation_free = \"\".join([i for i in text if i not in string.punctuation])\n",
        "        return punctuation_free\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        text = ' '.join([word for word in text.split() if word not in stopwords])\n",
        "        return text\n",
        "\n",
        "    def lemmatizer(self, text):\n",
        "        lemm_text = ' '.join([self.wordnet_lemmatizer.lemmatize(word) for word in text.split()])\n",
        "        return lemm_text\n",
        "\n",
        "    def tokenize(self, ds):\n",
        "        return self.tokenizer(ds[\"review\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "    def process_output(self, preds):\n",
        "        logger.info(\"Processing model predictions\")\n",
        "        rating_preds = []\n",
        "        for i in preds[\"logits\"]:\n",
        "            rating_preds.append(np.argmax(i, axis=0))\n",
        "\n",
        "        logger.info(\"Create output array for predictions\")\n",
        "        rating_preds = np.array(rating_preds)\n",
        "\n",
        "        return rating_preds\n",
        "\n",
        "    def process_whole(self, text):\n",
        "        tf_inf = self.preprocess_text(text, feature_names=None)\n",
        "        logger.info(\"Predictions ready to be made\")\n",
        "        preds = self.model.predict(tf_inf)\n",
        "        logger.info(f\"Prediction type: {type(preds)}\")\n",
        "        logger.info(f\"Predictions: {preds}\")\n",
        "        preds_proc = self.process_output(preds)\n",
        "        logger.info(f\"Processed predictions: {preds_proc}, Processed predictions type: {type(preds_proc)}\")\n",
        "\n",
        "        return preds_proc\n",
        "\n",
        "    def predict(self, text, names=[], meta=[]):\n",
        "        try:\n",
        "            if not self.ready:\n",
        "                self.load_model()\n",
        "                logger.info(\"Model successfully loaded\")\n",
        "                self.ready = True\n",
        "                logger.info(f\"{self.model.summary}\")\n",
        "                pred_proc = self.process_whole(text)\n",
        "            else:\n",
        "                pred_proc = self.process_whole(text)\n",
        "\n",
        "            return pred_proc\n",
        "\n",
        "        except Exception as ex:\n",
        "            logging.exception(f\"Failed during predict: {ex}\")"
      ],
      "metadata": {
        "id": "DAuwbV2bhJgz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I-j7PE98hMMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}